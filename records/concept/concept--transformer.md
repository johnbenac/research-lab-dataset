---
id: concept:transformer
datasetId: dataset:research-lab
typeId: concept
createdAt: 2025-12-20T00:00:00Z
updatedAt: 2025-12-27T00:00:00Z
fields:
  name: Transformer
  links:
    - paper:attention-is-all-you-need
---
Definition:
A neural architecture based primarily on attention mechanisms, often referenced via [[paper:attention-is-all-you-need]].
